{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "174feb03",
   "metadata": {},
   "source": [
    "# Verify the segments are correct"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de55e3fb",
   "metadata": {},
   "source": [
    "Create a fast binary classifier that determines if a frame has interpreter or not. Can't use the face_recognizer as it's not batched and is very slow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "28480204",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "notebook_dir = os.getcwd()\n",
    "project_root = os.path.abspath(os.path.join(notebook_dir, '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# Interpreter box\n",
    "X, Y = 571, 208\n",
    "W, H = 180, 193\n",
    "ROOT_DIR = 'scraped'"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd9a010c",
   "metadata": {},
   "source": [
    "## Extract frames with and without interpreter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "962c5a08",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "561\n",
      "22440\n",
      "22440\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "import cv2\n",
    "import numpy as np\n",
    "import os\n",
    "\n",
    "from utils.video import get_vid_metadata, get_frame\n",
    "\n",
    "\n",
    "def pick_indices(count, start, end, n_inside=40, n_outside=40):\n",
    "    # 1) Inside [start, end]\n",
    "    inside = np.linspace(start, end, n_inside, dtype=int)\n",
    "\n",
    "    # 2) Outside: [0, start) ∪ (end, count-1]\n",
    "    left_len = max(0, start)  # indices 0 .. start-1  (length = start)\n",
    "    right_len = max(0, count - 1 - end)  # indices end+1 .. count-1\n",
    "\n",
    "    total_outside_len = left_len + right_len\n",
    "    if total_outside_len == 0 or n_outside == 0:\n",
    "        outside = np.array([], dtype=int)\n",
    "        return inside, outside\n",
    "\n",
    "    # Allocate samples proportionally to segment length\n",
    "    if left_len == 0:\n",
    "        n_left = 0\n",
    "        n_right = n_outside\n",
    "    elif right_len == 0:\n",
    "        n_left = n_outside\n",
    "        n_right = 0\n",
    "    else:\n",
    "        n_left = int(round(n_outside * left_len / total_outside_len))\n",
    "        n_left = min(n_left, n_outside)  # safety\n",
    "        n_right = n_outside - n_left\n",
    "\n",
    "    # Sample each side with linspace (uniform within each segment)\n",
    "    left = (\n",
    "        np.linspace(0, start - 1, n_left, dtype=int)\n",
    "        if n_left > 0 and left_len > 0\n",
    "        else np.array([], dtype=int)\n",
    "    )\n",
    "    right = (\n",
    "        np.linspace(end + 1, count - 1, n_right, dtype=int)\n",
    "        if n_right > 0 and right_len > 0\n",
    "        else np.array([], dtype=int)\n",
    "    )\n",
    "\n",
    "    # Combine & ensure sorted, unique (dedup if linspace collapsed values)\n",
    "    outside = np.unique(np.concatenate([left, right])).astype(int)\n",
    "\n",
    "    assert len(inside) == n_inside and len(outside) == n_outside\n",
    "\n",
    "    return inside, outside\n",
    "\n",
    "\n",
    "with_interpreter = []\n",
    "without_interpreter = []\n",
    "\n",
    "\n",
    "i = 0\n",
    "for root, _, files in os.walk(ROOT_DIR):\n",
    "    for file in files:\n",
    "        if not file.endswith('.json'):\n",
    "            continue\n",
    "        json_path = os.path.join(root, file)\n",
    "        vid_path = json_path.replace('.json', '.mp4')\n",
    "\n",
    "        with open(json_path) as f:\n",
    "            js = json.load(f)\n",
    "\n",
    "        start = js['start']\n",
    "        end = js['end']\n",
    "\n",
    "        cap, _, frame_count = get_vid_metadata(vid_path)\n",
    "        cap.release()\n",
    "\n",
    "        with_interpreter_frame_indexes, without_interpreter_frame_indexes = (\n",
    "            pick_indices(frame_count, start, end)\n",
    "        )\n",
    "\n",
    "        cap = cv2.VideoCapture(vid_path)\n",
    "        if not cap.isOpened():\n",
    "            raise IOError(f'Could not open video: {vid_path}')\n",
    "\n",
    "        for w in with_interpreter_frame_indexes:\n",
    "            with_interpreter.append(get_frame(w, cap, X, Y, H, W))\n",
    "\n",
    "        for wo in without_interpreter_frame_indexes:\n",
    "            without_interpreter.append(get_frame(wo, cap, X, Y, H, W))\n",
    "\n",
    "        cap.release()\n",
    "        i += 1\n",
    "\n",
    "\n",
    "print(i)\n",
    "print(len(with_interpreter))\n",
    "print(len(without_interpreter))\n",
    "\n",
    "\n",
    "with_interpreter = np.array(with_interpreter)\n",
    "np.save('with_interpreter.npy', with_interpreter)\n",
    "without_interpreter = np.array(without_interpreter)\n",
    "np.save('without_interpreter.npy', without_interpreter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13a5e1d6",
   "metadata": {},
   "source": [
    "## Train fast binary classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "ac9f083e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_net(\n",
    "    with_interpreter,\n",
    "    without_interpreter,\n",
    "    epochs: int = 6,\n",
    "    batch_size: int = 512,\n",
    "    lr: float = 1e-3,\n",
    "    save_path: str = 'binary_classifier.pt',\n",
    "):  # ────────────────────────────────────────────────────────────────────\n",
    "    # 1. Imports\n",
    "    # ────────────────────────────────────────────────────────────────────\n",
    "    import itertools\n",
    "    import os\n",
    "    import time\n",
    "\n",
    "    import torch\n",
    "    from PIL import Image\n",
    "    from torch import nn\n",
    "    from torch.utils.data import ConcatDataset, DataLoader, Dataset\n",
    "    from torchvision import models\n",
    "    from torchvision import transforms as T\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    # ────────────────────────────────────────────────────────────────────\n",
    "    # 2.  Dataset\n",
    "    # ────────────────────────────────────────────────────────────────────\n",
    "    weights_enum = models.MobileNet_V3_Small_Weights.DEFAULT\n",
    "    try:\n",
    "        mean = weights_enum.meta['mean']\n",
    "        std = weights_enum.meta['std']\n",
    "    except (KeyError, AttributeError):\n",
    "        # Standard ImageNet normalisation\n",
    "        mean = (0.485, 0.456, 0.406)\n",
    "        std = (0.229, 0.224, 0.225)\n",
    "\n",
    "    class FrameDataset(Dataset):\n",
    "        def __init__(self, frames, label):\n",
    "            self.tfm = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n",
    "            self.label = label\n",
    "            self.data = frames\n",
    "\n",
    "        def __len__(self):\n",
    "            return len(self.data)\n",
    "\n",
    "        def __getitem__(self, idx):\n",
    "            img = self.data[idx]\n",
    "            pil = Image.fromarray(img)\n",
    "            return self.tfm(pil), self.label\n",
    "\n",
    "    ds_int = FrameDataset(with_interpreter, 1)\n",
    "    ds_no = FrameDataset(without_interpreter, 0)\n",
    "\n",
    "    full_ds = ConcatDataset([ds_int, ds_no])\n",
    "    loader = DataLoader(\n",
    "        full_ds,\n",
    "        batch_size=batch_size,\n",
    "        shuffle=True,\n",
    "        num_workers=min(4, os.cpu_count() or 1),\n",
    "        pin_memory=torch.cuda.is_available(),\n",
    "    )\n",
    "\n",
    "    # ────────────────────────────────────────────────────────────────────\n",
    "    # 3.  Model – MobileNet V3 Small (backbone frozen)\n",
    "    # ────────────────────────────────────────────────────────────────────\n",
    "    model = models.mobilenet_v3_small(weights=weights_enum)\n",
    "    in_feats = model.classifier[3].in_features\n",
    "    model.classifier[3] = nn.Linear(in_feats, 2)  # type: ignore\n",
    "\n",
    "    for p in itertools.chain(model.features.parameters(), model.avgpool.parameters()):\n",
    "        p.requires_grad = False\n",
    "    model = model.to(device)\n",
    "\n",
    "    # ────────────────────────────────────────────────────────────────────\n",
    "    # 4.  Loss, optimiser, early stop\n",
    "    # ────────────────────────────────────────────────────────────────────\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    optimizer = torch.optim.AdamW(model.classifier.parameters(), lr=lr)\n",
    "\n",
    "    best_acc, patience = 0.0, 3\n",
    "    start_time = time.time()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        for x, y in loader:\n",
    "            x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "            logits = model(x)\n",
    "            loss = criterion(logits, y)\n",
    "            optimizer.zero_grad(set_to_none=True)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        # quick accuracy on same loader (enough for early stopping)\n",
    "        model.eval()\n",
    "        correct, total = 0, 0\n",
    "        with torch.no_grad():\n",
    "            for x, y in loader:\n",
    "                x, y = x.to(device, non_blocking=True), y.to(device, non_blocking=True)\n",
    "                pred = model(x).argmax(1)\n",
    "                correct += (pred == y).sum().item()\n",
    "                total += y.numel()\n",
    "        acc = correct / total\n",
    "        print(f'Epoch {epoch + 1:02d}: accuracy={acc:.4f}')\n",
    "\n",
    "        if acc > best_acc:\n",
    "            best_acc, patience = acc, 3\n",
    "        else:\n",
    "            patience -= 1\n",
    "        if patience == 0 or best_acc >= 0.995:\n",
    "            break\n",
    "\n",
    "    print(\n",
    "        f'Training finished in {time.time() - start_time:.1f}s '\n",
    "        f'with accuracy {best_acc:.4f}'\n",
    "    )\n",
    "\n",
    "    # ────────────────────────────────────────────────────────────────────\n",
    "    # 5.  Save weights\n",
    "    # ────────────────────────────────────────────────────────────────────\n",
    "    torch.save(model.state_dict(), save_path)\n",
    "    print(f'Model saved to {os.path.abspath(save_path)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "8083cf4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 01: accuracy=0.9824\n",
      "Epoch 02: accuracy=0.9941\n",
      "Epoch 03: accuracy=0.9985\n",
      "Training finished in 82.9s with accuracy 0.9985\n",
      "Model saved to /home/radumicea/Projects/University/LinguSign/protv/binary_classifier.pt\n"
     ]
    }
   ],
   "source": [
    "train_net(with_interpreter, without_interpreter)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e651b97d",
   "metadata": {},
   "source": [
    "## Test the cropped segments to only have interpreter frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ab669e9f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([52])\n",
      "DONE\n",
      "torch.Size([41])\n",
      "DONE\n",
      "torch.Size([62])\n",
      "DONE\n",
      "torch.Size([9])\n",
      "DONE\n",
      "torch.Size([66])\n",
      "DONE\n",
      "torch.Size([29])\n",
      "DONE\n",
      "torch.Size([23])\n",
      "DONE\n",
      "torch.Size([107])\n",
      "DONE\n",
      "torch.Size([29])\n",
      "DONE\n",
      "torch.Size([48])\n",
      "DONE\n",
      "torch.Size([52])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([6])\n",
      "DONE\n",
      "torch.Size([77])\n",
      "DONE\n",
      "torch.Size([5])\n",
      "DONE\n",
      "torch.Size([8])\n",
      "DONE\n",
      "torch.Size([71])\n",
      "DONE\n",
      "torch.Size([125])\n",
      "DONE\n",
      "torch.Size([234])\n",
      "scraped/2025/05/04/62540479-2.seg.mp4 at [74, 225]\n",
      "DONE\n",
      "torch.Size([56])\n",
      "DONE\n",
      "torch.Size([2])\n",
      "DONE\n",
      "torch.Size([5])\n",
      "DONE\n",
      "torch.Size([195])\n",
      "scraped/2025/05/17/62544360-2.seg.mp4 at [0, 175]\n",
      "DONE\n",
      "torch.Size([44])\n",
      "DONE\n",
      "torch.Size([129])\n",
      "DONE\n",
      "torch.Size([61])\n",
      "DONE\n",
      "torch.Size([2])\n",
      "DONE\n",
      "torch.Size([48])\n",
      "DONE\n",
      "torch.Size([9])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([7])\n",
      "DONE\n",
      "torch.Size([162])\n",
      "scraped/2025/10/15/62583074-2.seg.mp4 at [0, 150]\n",
      "DONE\n",
      "torch.Size([108])\n",
      "DONE\n",
      "torch.Size([8])\n",
      "DONE\n",
      "torch.Size([31])\n",
      "DONE\n",
      "torch.Size([19])\n",
      "DONE\n",
      "torch.Size([57])\n",
      "DONE\n",
      "torch.Size([65])\n",
      "DONE\n",
      "torch.Size([62])\n",
      "DONE\n",
      "torch.Size([49])\n",
      "DONE\n",
      "torch.Size([55])\n",
      "DONE\n",
      "torch.Size([14])\n",
      "DONE\n",
      "torch.Size([38])\n",
      "DONE\n",
      "torch.Size([57])\n",
      "DONE\n",
      "torch.Size([860])\n",
      "scraped/2025/10/27/62586423-2.seg.mp4 at [0, 148]\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([179])\n",
      "DONE\n",
      "torch.Size([5])\n",
      "DONE\n",
      "torch.Size([34])\n",
      "DONE\n",
      "torch.Size([80])\n",
      "DONE\n",
      "torch.Size([1])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([20])\n",
      "DONE\n",
      "torch.Size([45])\n",
      "DONE\n",
      "torch.Size([15])\n",
      "DONE\n",
      "torch.Size([30])\n",
      "DONE\n",
      "torch.Size([8])\n",
      "DONE\n",
      "torch.Size([6])\n",
      "DONE\n",
      "torch.Size([3])\n",
      "DONE\n",
      "torch.Size([9])\n",
      "DONE\n",
      "torch.Size([8])\n",
      "DONE\n",
      "torch.Size([27])\n",
      "DONE\n",
      "torch.Size([1])\n",
      "DONE\n",
      "torch.Size([51])\n",
      "DONE\n",
      "torch.Size([17])\n",
      "DONE\n",
      "torch.Size([89])\n",
      "DONE\n",
      "torch.Size([73])\n",
      "DONE\n",
      "torch.Size([57])\n",
      "DONE\n",
      "torch.Size([58])\n",
      "DONE\n",
      "torch.Size([48])\n",
      "DONE\n",
      "torch.Size([9])\n",
      "DONE\n",
      "torch.Size([54])\n",
      "DONE\n",
      "torch.Size([22])\n",
      "DONE\n",
      "torch.Size([24])\n",
      "DONE\n",
      "torch.Size([3])\n",
      "DONE\n",
      "torch.Size([59])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([63])\n",
      "DONE\n",
      "torch.Size([19])\n",
      "DONE\n",
      "torch.Size([61])\n",
      "DONE\n",
      "torch.Size([2])\n",
      "DONE\n",
      "torch.Size([29])\n",
      "DONE\n",
      "torch.Size([27])\n",
      "DONE\n",
      "torch.Size([78])\n",
      "DONE\n",
      "torch.Size([19])\n",
      "DONE\n",
      "torch.Size([41])\n",
      "DONE\n",
      "torch.Size([30])\n",
      "DONE\n",
      "torch.Size([2])\n",
      "DONE\n",
      "torch.Size([86])\n",
      "DONE\n",
      "torch.Size([48])\n",
      "DONE\n",
      "torch.Size([1])\n",
      "DONE\n",
      "torch.Size([64])\n",
      "DONE\n",
      "torch.Size([50])\n",
      "DONE\n",
      "torch.Size([2])\n",
      "DONE\n",
      "torch.Size([6])\n",
      "DONE\n",
      "torch.Size([13])\n",
      "DONE\n",
      "torch.Size([21])\n",
      "DONE\n",
      "torch.Size([87])\n",
      "DONE\n",
      "torch.Size([20])\n",
      "DONE\n",
      "torch.Size([25])\n",
      "DONE\n",
      "torch.Size([3])\n",
      "DONE\n",
      "torch.Size([19])\n",
      "DONE\n",
      "torch.Size([76])\n",
      "DONE\n",
      "torch.Size([28])\n",
      "DONE\n",
      "torch.Size([53])\n",
      "DONE\n",
      "torch.Size([28])\n",
      "DONE\n",
      "torch.Size([17])\n",
      "DONE\n",
      "torch.Size([113])\n",
      "DONE\n",
      "torch.Size([74])\n",
      "DONE\n",
      "torch.Size([207])\n",
      "DONE\n",
      "torch.Size([24])\n",
      "DONE\n",
      "torch.Size([77])\n",
      "DONE\n",
      "torch.Size([61])\n",
      "DONE\n",
      "torch.Size([151])\n",
      "DONE\n",
      "torch.Size([267])\n",
      "DONE\n",
      "torch.Size([60])\n",
      "DONE\n",
      "torch.Size([191])\n",
      "DONE\n",
      "torch.Size([30])\n",
      "DONE\n",
      "torch.Size([251])\n",
      "DONE\n",
      "torch.Size([38])\n",
      "DONE\n",
      "torch.Size([240])\n",
      "DONE\n",
      "torch.Size([141])\n",
      "DONE\n",
      "torch.Size([209])\n",
      "DONE\n",
      "torch.Size([177])\n",
      "DONE\n",
      "torch.Size([292])\n",
      "DONE\n",
      "torch.Size([234])\n",
      "DONE\n",
      "torch.Size([33])\n",
      "DONE\n",
      "torch.Size([140])\n",
      "DONE\n",
      "torch.Size([316])\n",
      "scraped/2025/01/14/62511907-2.seg.mp4 at [61, 194]\n",
      "DONE\n",
      "torch.Size([67])\n",
      "DONE\n",
      "torch.Size([287])\n",
      "DONE\n",
      "torch.Size([42])\n",
      "DONE\n",
      "torch.Size([130])\n",
      "scraped/2025/01/02/62509157-2.seg.mp4 at [7, 34]\n",
      "DONE\n",
      "torch.Size([389])\n",
      "DONE\n",
      "torch.Size([52])\n",
      "DONE\n",
      "torch.Size([54])\n",
      "DONE\n",
      "torch.Size([267])\n",
      "DONE\n",
      "torch.Size([262])\n",
      "DONE\n",
      "torch.Size([322])\n",
      "scraped/2025/01/17/62512780-2.seg.mp4 at [134, 278]\n",
      "DONE\n",
      "torch.Size([12])\n",
      "DONE\n",
      "torch.Size([325])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([26])\n",
      "DONE\n",
      "torch.Size([47])\n",
      "DONE\n",
      "torch.Size([17])\n",
      "DONE\n",
      "torch.Size([33])\n",
      "DONE\n",
      "torch.Size([46])\n",
      "DONE\n",
      "torch.Size([25])\n",
      "DONE\n",
      "torch.Size([4])\n",
      "DONE\n",
      "torch.Size([59])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([60])\n",
      "DONE\n",
      "torch.Size([73])\n",
      "DONE\n",
      "torch.Size([6])\n",
      "DONE\n",
      "torch.Size([39])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([42])\n",
      "DONE\n",
      "torch.Size([39])\n",
      "DONE\n",
      "torch.Size([44])\n",
      "DONE\n",
      "torch.Size([44])\n",
      "DONE\n",
      "torch.Size([41])\n",
      "DONE\n",
      "torch.Size([92])\n",
      "DONE\n",
      "torch.Size([35])\n",
      "DONE\n",
      "torch.Size([66])\n",
      "DONE\n",
      "torch.Size([26])\n",
      "DONE\n",
      "torch.Size([62])\n",
      "DONE\n",
      "torch.Size([78])\n",
      "DONE\n",
      "torch.Size([2])\n",
      "DONE\n",
      "torch.Size([24])\n",
      "DONE\n",
      "torch.Size([28])\n",
      "DONE\n",
      "torch.Size([179])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([10])\n",
      "DONE\n",
      "torch.Size([39])\n",
      "DONE\n",
      "torch.Size([7])\n",
      "DONE\n",
      "torch.Size([75])\n",
      "DONE\n",
      "torch.Size([14])\n",
      "DONE\n",
      "torch.Size([18])\n",
      "DONE\n",
      "torch.Size([38])\n",
      "DONE\n",
      "torch.Size([47])\n",
      "DONE\n",
      "torch.Size([154])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([3])\n",
      "DONE\n",
      "torch.Size([6])\n",
      "DONE\n",
      "torch.Size([47])\n",
      "DONE\n",
      "torch.Size([9])\n",
      "DONE\n",
      "torch.Size([7])\n",
      "DONE\n",
      "torch.Size([8])\n",
      "DONE\n",
      "torch.Size([9])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([6])\n",
      "DONE\n",
      "torch.Size([38])\n",
      "DONE\n",
      "torch.Size([1])\n",
      "DONE\n",
      "torch.Size([44])\n",
      "DONE\n",
      "torch.Size([45])\n",
      "DONE\n",
      "torch.Size([5])\n",
      "DONE\n",
      "torch.Size([47])\n",
      "DONE\n",
      "torch.Size([20])\n",
      "DONE\n",
      "torch.Size([38])\n",
      "DONE\n",
      "torch.Size([1])\n",
      "DONE\n",
      "torch.Size([40])\n",
      "DONE\n",
      "torch.Size([23])\n",
      "DONE\n",
      "torch.Size([81])\n",
      "DONE\n",
      "torch.Size([49])\n",
      "DONE\n",
      "torch.Size([71])\n",
      "DONE\n",
      "torch.Size([66])\n",
      "DONE\n",
      "torch.Size([308])\n",
      "scraped/2025/08/15/62567357-2.seg.mp4 at [22, 154]\n",
      "DONE\n",
      "torch.Size([44])\n",
      "DONE\n",
      "torch.Size([105])\n",
      "DONE\n",
      "torch.Size([55])\n",
      "DONE\n",
      "torch.Size([22])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([24])\n",
      "DONE\n",
      "torch.Size([26])\n",
      "DONE\n",
      "torch.Size([79])\n",
      "DONE\n",
      "torch.Size([45])\n",
      "DONE\n",
      "torch.Size([7])\n",
      "DONE\n",
      "torch.Size([75])\n",
      "DONE\n",
      "torch.Size([45])\n",
      "DONE\n",
      "torch.Size([13])\n",
      "DONE\n",
      "torch.Size([65])\n",
      "DONE\n",
      "torch.Size([16])\n",
      "DONE\n",
      "torch.Size([7])\n",
      "DONE\n",
      "torch.Size([43])\n",
      "DONE\n",
      "torch.Size([1])\n",
      "DONE\n",
      "torch.Size([32])\n",
      "DONE\n",
      "torch.Size([69])\n",
      "DONE\n",
      "torch.Size([9])\n",
      "DONE\n",
      "torch.Size([24])\n",
      "DONE\n",
      "torch.Size([11])\n",
      "DONE\n",
      "torch.Size([1364])\n",
      "scraped/2025/04/22/62537267-2.seg.mp4 at [9, 392]\n",
      "DONE\n",
      "torch.Size([49])\n",
      "DONE\n",
      "torch.Size([38])\n",
      "DONE\n",
      "torch.Size([19])\n",
      "DONE\n",
      "torch.Size([1])\n",
      "DONE\n",
      "torch.Size([1])\n",
      "DONE\n",
      "torch.Size([49])\n",
      "DONE\n",
      "torch.Size([40])\n",
      "DONE\n",
      "torch.Size([36])\n",
      "DONE\n",
      "torch.Size([30])\n",
      "DONE\n",
      "torch.Size([19])\n",
      "DONE\n",
      "torch.Size([65])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([42])\n",
      "DONE\n",
      "torch.Size([36])\n",
      "DONE\n",
      "torch.Size([38])\n",
      "DONE\n",
      "torch.Size([2])\n",
      "DONE\n",
      "torch.Size([7])\n",
      "DONE\n",
      "torch.Size([47])\n",
      "DONE\n",
      "torch.Size([66])\n",
      "DONE\n",
      "torch.Size([44])\n",
      "DONE\n",
      "torch.Size([47])\n",
      "DONE\n",
      "torch.Size([45])\n",
      "DONE\n",
      "torch.Size([29])\n",
      "DONE\n",
      "torch.Size([8])\n",
      "DONE\n",
      "torch.Size([15])\n",
      "DONE\n",
      "torch.Size([38])\n",
      "DONE\n",
      "torch.Size([12])\n",
      "DONE\n",
      "torch.Size([19])\n",
      "DONE\n",
      "torch.Size([31])\n",
      "DONE\n",
      "torch.Size([55])\n",
      "DONE\n",
      "torch.Size([14])\n",
      "DONE\n",
      "torch.Size([45])\n",
      "DONE\n",
      "torch.Size([41])\n",
      "DONE\n",
      "torch.Size([44])\n",
      "DONE\n",
      "torch.Size([44])\n",
      "DONE\n",
      "torch.Size([66])\n",
      "DONE\n",
      "torch.Size([17])\n",
      "DONE\n",
      "torch.Size([92])\n",
      "DONE\n",
      "torch.Size([56])\n",
      "DONE\n",
      "torch.Size([22])\n",
      "DONE\n",
      "torch.Size([4])\n",
      "DONE\n",
      "torch.Size([11])\n",
      "DONE\n",
      "torch.Size([79])\n",
      "DONE\n",
      "torch.Size([48])\n",
      "DONE\n",
      "torch.Size([15])\n",
      "DONE\n",
      "torch.Size([10])\n",
      "DONE\n",
      "torch.Size([97])\n",
      "DONE\n",
      "torch.Size([32])\n",
      "DONE\n",
      "torch.Size([35])\n",
      "DONE\n",
      "torch.Size([34])\n",
      "DONE\n",
      "torch.Size([8])\n",
      "DONE\n",
      "torch.Size([54])\n",
      "DONE\n",
      "torch.Size([38])\n",
      "DONE\n",
      "torch.Size([11])\n",
      "DONE\n",
      "torch.Size([42])\n",
      "DONE\n",
      "torch.Size([137])\n",
      "DONE\n",
      "torch.Size([113])\n",
      "scraped/2025/07/07/62558182-2.seg.mp4 at [0, 28]\n",
      "DONE\n",
      "torch.Size([175])\n",
      "scraped/2024/05/05/62446818-2.seg.mp4 at [114, 139]\n",
      "DONE\n",
      "torch.Size([180])\n",
      "scraped/2024/05/22/62451331-2.seg.mp4 at [4, 177]\n",
      "DONE\n",
      "torch.Size([211])\n",
      "DONE\n",
      "torch.Size([9])\n",
      "DONE\n",
      "torch.Size([3])\n",
      "DONE\n",
      "torch.Size([176])\n",
      "scraped/2024/05/21/62451022-2.seg.mp4 at [0, 166]\n",
      "DONE\n",
      "torch.Size([30])\n",
      "DONE\n",
      "torch.Size([12])\n",
      "DONE\n",
      "torch.Size([1])\n",
      "DONE\n",
      "torch.Size([5])\n",
      "DONE\n",
      "torch.Size([61])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([33])\n",
      "DONE\n",
      "torch.Size([1])\n",
      "DONE\n",
      "torch.Size([1])\n",
      "DONE\n",
      "torch.Size([9])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([9])\n",
      "DONE\n",
      "torch.Size([27])\n",
      "DONE\n",
      "torch.Size([1])\n",
      "DONE\n",
      "torch.Size([24])\n",
      "DONE\n",
      "torch.Size([33])\n",
      "DONE\n",
      "torch.Size([5])\n",
      "DONE\n",
      "torch.Size([44])\n",
      "DONE\n",
      "torch.Size([10])\n",
      "DONE\n",
      "torch.Size([18])\n",
      "DONE\n",
      "torch.Size([9])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([23])\n",
      "DONE\n",
      "torch.Size([4])\n",
      "DONE\n",
      "torch.Size([48])\n",
      "DONE\n",
      "torch.Size([5])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([23])\n",
      "DONE\n",
      "torch.Size([9])\n",
      "DONE\n",
      "torch.Size([1])\n",
      "DONE\n",
      "torch.Size([83])\n",
      "DONE\n",
      "torch.Size([5])\n",
      "DONE\n",
      "torch.Size([38])\n",
      "DONE\n",
      "torch.Size([2])\n",
      "DONE\n",
      "torch.Size([39])\n",
      "DONE\n",
      "torch.Size([53])\n",
      "DONE\n",
      "torch.Size([10])\n",
      "DONE\n",
      "torch.Size([26])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([1])\n",
      "DONE\n",
      "torch.Size([18])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([2])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([1])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([42])\n",
      "DONE\n",
      "torch.Size([43])\n",
      "DONE\n",
      "torch.Size([8])\n",
      "DONE\n",
      "torch.Size([26])\n",
      "DONE\n",
      "torch.Size([1])\n",
      "DONE\n",
      "torch.Size([138])\n",
      "DONE\n",
      "torch.Size([3])\n",
      "DONE\n",
      "torch.Size([6])\n",
      "DONE\n",
      "torch.Size([16])\n",
      "DONE\n",
      "torch.Size([29])\n",
      "DONE\n",
      "torch.Size([1])\n",
      "DONE\n",
      "torch.Size([1])\n",
      "DONE\n",
      "torch.Size([37])\n",
      "DONE\n",
      "torch.Size([219])\n",
      "DONE\n",
      "torch.Size([61])\n",
      "DONE\n",
      "torch.Size([75])\n",
      "DONE\n",
      "torch.Size([25])\n",
      "DONE\n",
      "torch.Size([30])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([3])\n",
      "DONE\n",
      "torch.Size([230])\n",
      "DONE\n",
      "torch.Size([74])\n",
      "DONE\n",
      "torch.Size([4])\n",
      "DONE\n",
      "torch.Size([16])\n",
      "DONE\n",
      "torch.Size([3])\n",
      "DONE\n",
      "torch.Size([11])\n",
      "DONE\n",
      "torch.Size([52])\n",
      "DONE\n",
      "torch.Size([47])\n",
      "DONE\n",
      "torch.Size([21])\n",
      "DONE\n",
      "torch.Size([93])\n",
      "DONE\n",
      "torch.Size([241])\n",
      "scraped/2024/12/22/62507025-2.seg.mp4 at [61, 168]\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([37])\n",
      "DONE\n",
      "torch.Size([30])\n",
      "DONE\n",
      "torch.Size([3])\n",
      "DONE\n",
      "torch.Size([43])\n",
      "DONE\n",
      "torch.Size([13])\n",
      "DONE\n",
      "torch.Size([16])\n",
      "DONE\n",
      "torch.Size([56])\n",
      "DONE\n",
      "torch.Size([1])\n",
      "DONE\n",
      "torch.Size([15])\n",
      "DONE\n",
      "torch.Size([28])\n",
      "DONE\n",
      "torch.Size([84])\n",
      "DONE\n",
      "torch.Size([272])\n",
      "scraped/2024/12/01/62502226-2.seg.mp4 at [0, 28]\n",
      "DONE\n",
      "torch.Size([3])\n",
      "DONE\n",
      "torch.Size([25])\n",
      "DONE\n",
      "torch.Size([17])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([54])\n",
      "scraped/2024/12/02/62502472-2.seg.mp4 at [1, 26]\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([3])\n",
      "DONE\n",
      "torch.Size([23])\n",
      "DONE\n",
      "torch.Size([48])\n",
      "DONE\n",
      "torch.Size([18])\n",
      "DONE\n",
      "torch.Size([1])\n",
      "DONE\n",
      "torch.Size([12])\n",
      "DONE\n",
      "torch.Size([4])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([944])\n",
      "scraped/2024/09/21/62484591-2.seg.mp4 at [189, 236]\n",
      "DONE\n",
      "torch.Size([1])\n",
      "DONE\n",
      "torch.Size([92])\n",
      "scraped/2024/09/23/62484890-2.seg.mp4 at [50, 83]\n",
      "DONE\n",
      "torch.Size([1])\n",
      "DONE\n",
      "torch.Size([8])\n",
      "DONE\n",
      "torch.Size([553])\n",
      "scraped/2024/09/12/62482413-2.seg.mp4 at [32, 176]\n",
      "DONE\n",
      "torch.Size([55])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([156])\n",
      "DONE\n",
      "torch.Size([2])\n",
      "DONE\n",
      "torch.Size([2])\n",
      "DONE\n",
      "torch.Size([18])\n",
      "DONE\n",
      "torch.Size([182])\n",
      "scraped/2024/09/11/62482212-2.seg.mp4 at [0, 160]\n",
      "DONE\n",
      "torch.Size([1])\n",
      "DONE\n",
      "torch.Size([11])\n",
      "DONE\n",
      "torch.Size([3])\n",
      "DONE\n",
      "torch.Size([7])\n",
      "DONE\n",
      "torch.Size([2])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([14])\n",
      "DONE\n",
      "torch.Size([8])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([363])\n",
      "scraped/2024/11/05/62496288-2.seg.mp4 at [0, 145]\n",
      "DONE\n",
      "torch.Size([9])\n",
      "DONE\n",
      "torch.Size([18])\n",
      "DONE\n",
      "torch.Size([171])\n",
      "scraped/2024/11/21/62499867-2.seg.mp4 at [0, 159]\n",
      "DONE\n",
      "torch.Size([118])\n",
      "DONE\n",
      "torch.Size([43])\n",
      "DONE\n",
      "torch.Size([12])\n",
      "DONE\n",
      "torch.Size([1])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([341])\n",
      "scraped/2024/11/24/62500583-2.seg.mp4 at [176, 333]\n",
      "DONE\n",
      "torch.Size([479])\n",
      "scraped/2024/11/20/62499552-2.seg.mp4 at [0, 153]\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([13])\n",
      "DONE\n",
      "torch.Size([64])\n",
      "scraped/2024/11/06/62496467-2.seg.mp4 at [4, 32]\n",
      "DONE\n",
      "torch.Size([37])\n",
      "DONE\n",
      "torch.Size([64])\n",
      "DONE\n",
      "torch.Size([59])\n",
      "DONE\n",
      "torch.Size([12])\n",
      "DONE\n",
      "torch.Size([757])\n",
      "scraped/2024/11/28/62501603-2.seg.mp4 at [2, 366]\n",
      "DONE\n",
      "torch.Size([715])\n",
      "scraped/2024/11/30/62501999-2.seg.mp4 at [0, 326]\n",
      "DONE\n",
      "torch.Size([13])\n",
      "DONE\n",
      "torch.Size([49])\n",
      "DONE\n",
      "torch.Size([8])\n",
      "DONE\n",
      "torch.Size([3])\n",
      "DONE\n",
      "torch.Size([5])\n",
      "DONE\n",
      "torch.Size([62])\n",
      "scraped/2024/06/10/62456181-2.seg.mp4 at [0, 29]\n",
      "DONE\n",
      "torch.Size([39])\n",
      "DONE\n",
      "torch.Size([271])\n",
      "DONE\n",
      "torch.Size([5])\n",
      "DONE\n",
      "torch.Size([37])\n",
      "DONE\n",
      "torch.Size([7])\n",
      "DONE\n",
      "torch.Size([13])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([23])\n",
      "DONE\n",
      "torch.Size([17])\n",
      "DONE\n",
      "torch.Size([205])\n",
      "scraped/2024/06/09/62455878-2.seg.mp4 at [0, 32]\n",
      "DONE\n",
      "torch.Size([39])\n",
      "DONE\n",
      "torch.Size([6])\n",
      "DONE\n",
      "torch.Size([13])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([50])\n",
      "DONE\n",
      "torch.Size([7])\n",
      "DONE\n",
      "torch.Size([119])\n",
      "DONE\n",
      "torch.Size([1])\n",
      "DONE\n",
      "torch.Size([5])\n",
      "DONE\n",
      "torch.Size([64])\n",
      "DONE\n",
      "torch.Size([13])\n",
      "DONE\n",
      "torch.Size([56])\n",
      "DONE\n",
      "torch.Size([7])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([3])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([5])\n",
      "DONE\n",
      "torch.Size([92])\n",
      "DONE\n",
      "torch.Size([3])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([4])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([118])\n",
      "scraped/2024/08/23/62477382-2.seg.mp4 at [41, 71]\n",
      "DONE\n",
      "torch.Size([27])\n",
      "DONE\n",
      "torch.Size([55])\n",
      "DONE\n",
      "torch.Size([6])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([15])\n",
      "DONE\n",
      "torch.Size([5])\n",
      "DONE\n",
      "torch.Size([22])\n",
      "DONE\n",
      "torch.Size([171])\n",
      "DONE\n",
      "torch.Size([8])\n",
      "DONE\n",
      "torch.Size([2])\n",
      "DONE\n",
      "torch.Size([8])\n",
      "DONE\n",
      "torch.Size([2])\n",
      "DONE\n",
      "torch.Size([36])\n",
      "DONE\n",
      "torch.Size([1])\n",
      "DONE\n",
      "torch.Size([33])\n",
      "DONE\n",
      "torch.Size([3])\n",
      "DONE\n",
      "torch.Size([1])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([61])\n",
      "DONE\n",
      "torch.Size([39])\n",
      "DONE\n",
      "torch.Size([6])\n",
      "DONE\n",
      "torch.Size([89])\n",
      "DONE\n",
      "torch.Size([2])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([14])\n",
      "DONE\n",
      "torch.Size([10])\n",
      "DONE\n",
      "torch.Size([37])\n",
      "DONE\n",
      "torch.Size([9])\n",
      "DONE\n",
      "torch.Size([9])\n",
      "DONE\n",
      "torch.Size([12])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([10])\n",
      "DONE\n",
      "torch.Size([9])\n",
      "DONE\n",
      "torch.Size([7])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([37])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([19])\n",
      "DONE\n",
      "torch.Size([5])\n",
      "DONE\n",
      "torch.Size([1])\n",
      "DONE\n",
      "torch.Size([17])\n",
      "DONE\n",
      "torch.Size([13])\n",
      "DONE\n",
      "torch.Size([13])\n",
      "DONE\n",
      "torch.Size([55])\n",
      "DONE\n",
      "torch.Size([4])\n",
      "DONE\n",
      "torch.Size([14])\n",
      "DONE\n",
      "torch.Size([29])\n",
      "DONE\n",
      "torch.Size([3])\n",
      "DONE\n",
      "torch.Size([6])\n",
      "DONE\n",
      "torch.Size([4])\n",
      "DONE\n",
      "torch.Size([370])\n",
      "scraped/2024/04/19/62443062-2.seg.mp4 at [114, 173]\n",
      "DONE\n",
      "torch.Size([3])\n",
      "DONE\n",
      "torch.Size([9])\n",
      "DONE\n",
      "torch.Size([61])\n",
      "DONE\n",
      "torch.Size([65])\n",
      "DONE\n",
      "torch.Size([9])\n",
      "DONE\n",
      "torch.Size([10])\n",
      "DONE\n",
      "torch.Size([47])\n",
      "DONE\n",
      "torch.Size([65])\n",
      "DONE\n",
      "torch.Size([3])\n",
      "DONE\n",
      "torch.Size([200])\n",
      "DONE\n",
      "torch.Size([14])\n",
      "DONE\n",
      "torch.Size([1])\n",
      "DONE\n",
      "torch.Size([7])\n",
      "DONE\n",
      "torch.Size([2])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([10])\n",
      "DONE\n",
      "torch.Size([15])\n",
      "DONE\n",
      "torch.Size([11])\n",
      "DONE\n",
      "torch.Size([66])\n",
      "DONE\n",
      "torch.Size([40])\n",
      "DONE\n",
      "torch.Size([2])\n",
      "DONE\n",
      "torch.Size([38])\n",
      "DONE\n",
      "torch.Size([20])\n",
      "DONE\n",
      "torch.Size([10])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n",
      "torch.Size([0])\n",
      "DONE\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from PIL import Image\n",
    "from torchvision import models\n",
    "from torchvision import transforms as T\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "import os\n",
    "import cv2\n",
    "\n",
    "\n",
    "def load_interpreter_model(model_path: str):\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "\n",
    "    weights_enum = models.MobileNet_V3_Small_Weights.DEFAULT\n",
    "    try:\n",
    "        mean = weights_enum.meta['mean']\n",
    "        std = weights_enum.meta['std']\n",
    "    except (KeyError, AttributeError):\n",
    "        # Standard ImageNet normalisation\n",
    "        mean = (0.485, 0.456, 0.406)\n",
    "        std = (0.229, 0.224, 0.225)\n",
    "\n",
    "    model = models.mobilenet_v3_small(weights=weights_enum)\n",
    "    in_feats = model.classifier[3].in_features\n",
    "    model.classifier[3] = nn.Linear(in_feats, 2)  # type: ignore\n",
    "    model.load_state_dict(torch.load(model_path, map_location=device))\n",
    "    model.eval().to(device)\n",
    "\n",
    "    transform = T.Compose([T.ToTensor(), T.Normalize(mean, std)])\n",
    "    return model, transform, device\n",
    "\n",
    "\n",
    "fps = 25\n",
    "model, tfm, dev = load_interpreter_model('binary_classifier.pt')\n",
    "\n",
    "\n",
    "@torch.inference_mode()\n",
    "def has_interpreter_batch(\n",
    "    frames: list[np.ndarray] | np.ndarray,\n",
    "    model: torch.nn.Module,\n",
    "    transform: T.Compose,\n",
    "    device: torch.device | str = 'cpu',\n",
    "    threshold: float = 0.5,\n",
    "    return_probs: bool = False,\n",
    "):\n",
    "    if isinstance(frames, np.ndarray) and frames.ndim == 4:\n",
    "        frames_iter = frames\n",
    "    else:\n",
    "        frames_iter = list(frames)\n",
    "\n",
    "    tensors = []\n",
    "    for frame in frames_iter:\n",
    "        pil = Image.fromarray(frame)\n",
    "        tensors.append(transform(pil))\n",
    "    batch = torch.stack(tensors).to(device)\n",
    "\n",
    "    logits = model(batch)\n",
    "    probs = torch.softmax(logits, dim=1)[:, 1]  # class-1 = interpreter\n",
    "    preds = (probs >= threshold).cpu()\n",
    "\n",
    "    if return_probs:\n",
    "        return preds, probs.cpu()\n",
    "    return preds\n",
    "\n",
    "\n",
    "def video_has_interpreter_always(video_path, batch_size=1024):\n",
    "    cap = cv2.VideoCapture(video_path)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Can't open {video_path}\")\n",
    "\n",
    "    wrongs = []\n",
    "\n",
    "    done = False\n",
    "    while not done:\n",
    "        frames_batch = []\n",
    "\n",
    "        for _ in range(batch_size):\n",
    "            ok, frame = cap.read()\n",
    "            if not ok:\n",
    "                done = True\n",
    "                break\n",
    "\n",
    "            frame = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
    "            frames_batch.append(frame)\n",
    "\n",
    "        if not frames_batch:\n",
    "            break\n",
    "\n",
    "        results = has_interpreter_batch(frames_batch, model, tfm, dev)\n",
    "        wrongs.append(torch.nonzero(~results, as_tuple=True)[0])  # type: ignore\n",
    "\n",
    "    x = torch.cat(wrongs)\n",
    "    print(x.shape)\n",
    "\n",
    "    if x.numel() < fps:\n",
    "        cap.release()\n",
    "        return\n",
    "\n",
    "    # Differences between adjacent elements\n",
    "    diff = x[1:] - x[:-1]\n",
    "\n",
    "    # True where the step is exactly +1 => consecutive integers\n",
    "    is_step1 = diff == 1\n",
    "\n",
    "    if is_step1.numel() == 0:\n",
    "        cap.release()\n",
    "        return\n",
    "\n",
    "    # Find runs of True in is_step1\n",
    "    # Pad with False at both ends so transitions show up at boundaries\n",
    "    pad = torch.tensor([False], device=is_step1.device)\n",
    "    p = torch.cat([pad, is_step1, pad])  # length = len(diff) + 2\n",
    "\n",
    "    # Locations where value changes (False->True or True->False)\n",
    "    changes = torch.nonzero(p[1:] != p[:-1]).flatten()\n",
    "    # True-runs start at even indices, end at odd indices in `changes`\n",
    "    starts = changes[0::2]  # indices into is_step1 where a run starts\n",
    "    ends = changes[1::2]  # first index AFTER the run in is_step1\n",
    "\n",
    "    # Lengths in terms of number of diffs (True values)\n",
    "    run_len_diffs = ends - starts\n",
    "\n",
    "    # Each run of k diffs corresponds to k+1 elements in x\n",
    "    run_len_elems = run_len_diffs + 1\n",
    "\n",
    "    # 4. Check for any run with at least `min_len` elements\n",
    "    ok = run_len_elems >= fps\n",
    "    if not ok.any():\n",
    "        cap.release()\n",
    "        return\n",
    "\n",
    "    # Get the first qualifying block\n",
    "    first_idx = torch.nonzero(ok, as_tuple=False)[0, 0]\n",
    "    start_in_diffs = starts[first_idx].item()\n",
    "    length_elems = run_len_elems[first_idx].item()\n",
    "\n",
    "    # In the original x, the block is [start_idx, end_idx_exclusive)\n",
    "    start_idx = start_in_diffs\n",
    "    end_idx_exclusive = start_in_diffs + length_elems\n",
    "\n",
    "    print(f'{video_path} at [{start_idx}, {end_idx_exclusive}]')\n",
    "\n",
    "    cap.release()\n",
    "\n",
    "\n",
    "for root, _, files in os.walk(ROOT_DIR):\n",
    "    for file in files:\n",
    "        if not file.endswith('.seg.mp4'):\n",
    "            continue\n",
    "        path = os.path.join(root, file)\n",
    "        video_has_interpreter_always(path)\n",
    "        print('DONE')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
